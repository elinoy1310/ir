# דוח עבודה – קורס אחזור מידע / תרגיל 1
**מגישות:** אלינוי דמרי 325984318, ריקי רובין 326380359 
**מספר קבוצת תרגיל:** <br> 
תיעוד מלא של שלבי עיבוד הנתונים, ההכנה ובניית הייצוגים למסמכי הפרלמנט הבריטי.
## תוכן עניינים
- [שלב 1 – הורדת קבצי XML](#שלב-1--הורדת-קבצי-xml)
- [שלב 2 – ניקוי קבצי הXML](#שלב-2--ניקוי-קבצי-הxml)
  - [שלב 2.1 – ניקוי תגיות XML](#שלב-21--ניקוי-תגיות-xml)
  - [שלב 2.2 – טוקניזציה (Tokenization)](#שלב-22--טוקניזציה-tokenization)
- [שלב 3 – Lemmatization](#שלב-3--lemmatization)
- [שלב 4 – בניית וקטורים לייצוג TF-IDF עם BM25\Okapi](#שלב-4---בניית-וקטורים-לייצוג-tf-idf-עם-bm25okapi)
- [שלב 5 – בניית ייצוגים סמנטיים (Word2Vec)](#שלב-5--בניית-ייצוגים-סמנטיים-word2vec)
- [שלב 6 – יצירת ייצוגים סמנטיים בעזרת SimCSE ו-SBERT](#שלב-6--יצירת-ייצוגים-סמנטים-למסמכים-בעזרת-simcse-ו-sbert)
- [שלב 7 – חישוב Information Gain ו-Mutual Information](#שלב-7--חישוב-information-gain-ו-mutual-information)

---
## שלב 1 – הורדת קבצי XML
**מטרה:** לאסוף את כל מסמכי המקור הגולמיים (XML) כדי לעבד אותם בהמשך.

**ביצוע:**  
נכתב סקריפט (download_debates.py) המבצע הורדה אוטומטית של כל קובץ XML מהאתר: https://www.theyworkforyou.com/pwdata/scrapedxml/debates/ החל מהקובץ debates2023-06-28d.xml ועד סוף הארכיון. <br> 
הסקריפט שולף את רשימת הקבצים מהאתר הרשמי, מסנן לפי תבנית שם, ומוריד כל מסמך לתיקייה מסודרת. 

**תוצאות:**  
התקבלה ספרייה בשם debates_xml שמכילה 956 קבצים.

---

## שלב 2 – ניקוי קבצי הXML
### שלב 2.1 – ניקוי תגיות XML
**מטרה:** להפיק טקסט נקי מתגיות ולהבטיח שרק קבצים בעלי תוכן יעברו לעיבוד.

**ביצוע:**  
נכתב סקריפט (clean_tags.py) שמעבד את קבצי הxml בעזרת הספרייה `lxml` ומשאיר רק את הטקסט שהיה בין התגיות. 
לאחר השלמת פעולת הניקוי הורץ קוד שבודק כמה קבצים ריקים יש - אין בהן תוכן, ואת שמותיהם ואותם מחקנו כי לא נותנים שום מידע.  

**תוצאות:**  
התקבלה ספרייה בשם clean_text שמכילה את הקבצים הנקיים מתגים בלי 13 קבצים הריקים ששמותיהם:

 debates2023-11-14b.txt <br>
 debates2023-12-13b.txt <br>
 debates2023-12-18b.txt <br>
 debates2024-02-06c.txt <br>
 debates2024-05-23b.txt <br>
 debates2024-05-24b.txt <br>
 debates2024-07-17e.txt <br>
 debates2024-12-12b.txt <br>
 debates2025-01-17c.txt <br>
 debates2025-01-22a.txt <br>
 debates2025-03-20a.txt <br>
 debates2025-07-21c.txt <br>
 debates2025-07-22c.txt <br>

 סה"כ יש 943 קבצים בתיקייה לעיבוד עתידי.


### שלב 2.2 – טוקניזציה (Tokenization)
**מטרה:** לפרק את הטקסט למילים וסימני פיסוק נפרדים, כדי של מילה תהיה משמעות בפני עצמה ללא סימני פיסוק צמודים כך שיהיה אפשר לבצע חישובים בצורה מדוייקת.

**ביצוע:**  
נכתב סקריפט (tokenize_clean_text.py) שעובר על כל אחד מהקבצים שנוצרו בשלב 2.1 ומשתמש במנגנון המובנה של פייתון str.translate וב־string.punctuation כדי לזהות את כל סימני הפיסוק הנפוצים: !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~<br> 
לכל סימן פיסוק מוגדר תרגום המכניס רווח לפני ואחרי הסימן.
כך, בעת העיבוד, כל סימני הפיסוק מופרדים אוטומטית מהמילים.

**תוצאות:**  
התקבלה ספרייה בשם tokens שמכילה את הטקסט עם סימני פיסוק מופרדים.<br> 
הערה: מילים עם s השייכות שמופיעות כך: s' לא הופרדו כי זה חלק מהמילה ומציין שייכות ולא כמו סימני פיסוק אחרים שהם חלק מהמשפט ולא מהמילה. 


---

## שלב 3 – Lemmatization
**מטרה:** לצמצם וריאציות לשוניות על ידי המרה לצורות בסיס (lemmas), ולהקטין את מספר המאפיינים.

**ביצוע:**
כתיבת סקריפט (lemma.py) שעובר על כל הקבצים בתיקייה tokens ומשתמש בכלי spaCy עם המודל en_core_web_sm שמזהה את תפקיד המילה במשפט (POS) וכך עוזר ליצור את הלמה של המילה.

**תוצאות:**
התקבלה תיקייה lemmatized_text שהמילים בכל קובץ מופיעות בצורת הלמה שלהן.

בחרנו להשתמש ב-spaCy בשביל ליצור למות כי כי הוא יודע להחזיר את צורת השורש הנכונה של מילה לפי ההקשר במשפט,— הוא מזהה את חלקי המשפט ואת התפקיד של כל מילה ומחזיר צורה בסיסית נקייה שמתאימה לעיבוד נוסף. בנוסף הוא מהיר מאוד, יציב, ויכול לעבור על מאות מסמכים בלי בעיות. כלים אחרים כמו WordNet או TextBlob עובדים לפי מילונים ישנים ומחזירים לפעמים צורות לא מדויקות, ו-Snowball או Porter פשוט “חותכים” מילים ויוצרים מילים לא אמיתיות. Stanza גם מספק תוצאות טובות, אבל הוא כבד ואיטי בהרבה. לכן spaCy נותן את השילוב הנכון של איכות וביצועים, שמתאים לצינור העבודה שלנו.

---

## שלב 4 - בניית וקטורים לייצוג TF-IDF עם BM25\Okapi

**מטרה:**  
יצירת וקטור מאפיינים לכל מסמך. כל מסמך ממופה לווקטור מספרי המתאר את המילים שמופיעות בו ואת החשיבות היחסית שלהן באמצעות TF-IDF עם BM25\Okapi.

**ביצוע:**  
כתיבת סקריפט (`build_vectors_word.py`) שעובר על כל הקבצים פעמיים: פעם אחת בתיקייה `tokens` ופעם שנייה בתיקייה `lemmatized_text`. הסקריפט מבצע את הצעדים הבאים:

1. שמירת רשימה של טקסטים ורשימה של שמות הקבצים, כדי לשייך בין הווקטורים שנוצרים למסמכים.
2. המרת הטקסטים למטריצות שבהן כל תא מכיל את כמות הפעמים שהמילה מופיעה במסמך (TF) באמצעות הפונקציה `CountVectorizer` מתוך `scikit-learn`:

```python
from sklearn.feature_extraction.text import CountVectorizer

count_vec = CountVectorizer(
    input="content",
    analyzer="word",
    stop_words="english",
    min_df=5
)
```
| פרמטר                  | משמעות                             |
| ---------------------- | ---------------------------------- |
| `input="content"`      | מציין שהקלט הוא טקסט גולמי         |
| `analyzer="word"`      | מפצל את הטקסט למילים               |
| `stop_words="english"` | מסיר stopwords באנגלית             |
| `min_df=5`             | מסיר מילים שמופיעות פחות מ-5 פעמים |

3. חישוב כל תא במטריצה באמצעות BM25\Okapi ידנית עם הפרמטרים `k=1.5` ו-`b=0.75`.  
4. שמירת המטריצה שנוצרה בפורמט `.npz` כדי לשמור מטריצה דלילה.  
5. שמירת המילים עצמם בפורמט JSON, כאשר המפתח הוא המילה והערך הוא מספר הפעמים שהיא מופיעה.  
6. שמירת שמות הקבצים בפורמט JSON.
7. הרצת סקריפט check_matrix.py בשביל לקבל קצת מידע על המטריצה שנוצרה - כמה מאפיינים יש , כמה שורות ריקות (אם ישנן) , הצגת 10 המאפיינים עם סכום העמודה הגבוה ביותר ו5 השורות הראשונות.

בחרנו לכתוב את הקוד בצורה הזאת כי זה הפתרון הכי מסודר ויעיל , השתמשנו בכלים מוכנים של scikit-learn שמייצרים את המטריצות בצורה אוטומטית ומהירה, ובנינו פונקציות כלליות שאפשר להריץ גם על מילים רגילות וגם על למות בלי לשכפל קוד. בנוסף שמרנו את המטריצות במבנה דליל (sparse) כדי לחסוך מקום בזיכרון, כי רוב הערכים הם אפסים. מימוש BM25 נעשה ידנית כדי שתהיה לנו שליטה על הפרמטרים וכדי להבין באמת איך המשקל מחושב. בסוף זה נותן קוד ברור, יעיל וקל לתחזוקה.
---

**תוצאות:**  
לאחר שתי הרצות התקבלו שתי תיקיות: `vectors_word` ו-`vectors_lemm`, שבהן יש את הקבצים הבאים:

- **מטריצת TF-IDF:**  
   `TFIDF-Word.npz` / `TFIDF-Lemm.npz`

- **המילים בעמודות המטריצה:**  
   `TFIDF-Word_vocabulary.json` / `TFIDF-Lemm_vocabulary.json`

- **שמות המסמכים (השורות במטריצה):**  
   `TFIDF-Word_files.json` / `TFIDF-Lemm_files.json`

**ויזואליזציה של המטריצות שנוצרו:** <br> 
עבור קבצי המקור לאחר הניקוי :
- מספר המאפיינים: 29151
- מספר שורות ריקות:0
- הצצה לערכי המטריצה: <img width="1288" height="153" alt="image" src="https://github.com/user-attachments/assets/2c99b59f-38c6-434f-b923-c6c9b0c5f6e7" />
 
עבור קבצי הלמות :
- מספר המאפיינים: 22993
- מספר שורות ריקות:0
- הצצה לערכי המטריצה: <img width="1292" height="159" alt="image" src="https://github.com/user-attachments/assets/06b9032b-f2f8-4c9f-89be-22682b63974e"  />


---

## שלב 5 – בניית ייצוגים סמנטיים (Word2Vec)
**מטרה:** לייצג מסמכים לפי המשמעות, לא רק לפי שכיחות מילים.

**ביצוע:**
כתיבת סקריפט (build_doc_embeddings_w2v.py) שעובר על כל הקבצים פעמיים: פעם אחת בתיקייה `tokens` ופעם שנייה בתיקייה `lemmatized_text`. הסקריפט מבצע את הצעדים הבאים:

1. שמירת רשימה של טקסטים ורשימה של שמות הקבצים, כדי לשייך בין הווקטורים שנוצרים למסמכים.
2. פיצול הטקסט למילים וניקוי פעמיים: בפעם הראשונה- הוצאת סימני פיסוק, סינון מילים שמכילים ספרות, סינון טוקנים קצרים חסרי משמעות (למעט "i", "a" ) ובפעם השנייה כמו בפעם הראשונה אבל גם עם סינון stopwords באנגלית.
3. אימון המודל:
   1. הגדרות- גודל הוקטור: 300, גודל החלון: 5 , מתעלמים ממילים שמופיעות פחות מ5 פעמים, מבצעים 10 epochs.
   2. ``` model.build_vocab(sentences) ``` בניית אוצר מילים מתוך כל המילים המופיעות מספיק פעמים.
   3. אימון המודל - למידת embeddings לכל מילה בהתאם להקשר שלה (CBOW/Skip-gram).
      (המודל שנוצר מכיל וקטור צפוף לכל מילה שזוהתה.)
4.  יצירת וקטורי מסמך: לכל מסמך בודקים איזה מילים קיימות במודל הword2vec שאומן ולכל מילה שנמצאה לוקחים את הוקטור שלה וסוכמים עם שאר המילים שנמצאו ועושים ממוצע והתוצאה זה וקטור המסמך . <br>
הערה: אם המסמך לא מכיל מילים אחרי הסינון הוקטור שלו הוא וקטור ה0. 
5. שמירת המטריצה שנוצרה בפורמט `.npy` כדי לשמור מטריצה צפופה -> נעשה פעמיים : פעם אחת עם stopwords ופעם שנייה בלי stopwords. 
6. שמירת שמות הקבצים (= שורות המטריצה ) בפורמט JSON -> נעשה פעמיים : פעם אחת עם stopwords ופעם שנייה בלי stopwords.
7. הרצת סקריפט (check_matrix_w2v.py) בשביל לקבל קצת מידע על המטריצה שנוצרה - כמה שורות ריקות (אם ישנן) , הצגת 10 המאפיינים הראשונים מתוך 300 מאפיינים ו5 השורות הראשונות. 
   
**תוצאות:**
התקבלו הקבצים הבאים:
| קובץ / אובייקט         | תוכן                                          |
| ---------------------- | --------------------------------------------- |
| `W2V_Word_basic.npy`   | וקטורי מסמכים Word-level ללא הסרת stop-words  |
| `W2V_Word_nostop.npy`  | וקטורי מסמכים Word-level עם הסרת stop-words   |
| `W2V_Lemm_basic.npy`   | וקטורי מסמכים Lemma-level ללא הסרת stop-words |
| `W2V_Lemm_nostop.npy`  | וקטורי מסמכים Lemma-level עם הסרת stop-words  |
| `W2V_Word_files.json`     | שמות הקבצים ושיוך לשורות במטריצה              |
| `W2V_Lemm_files.json`     | שמות הקבצים ושיוך לשורות במטריצה              |
| `w2v_model_word.model` | מודל Word2Vec מאומן על Word-level             |
| `w2v_model_lemm.model` | מודל Word2Vec מאומן על Lemma-level            |

**ויזואליזציה של המטריצות שנוצרו:** <br> 
עבור קבצי המקור לאחר הניקוי עם stopwords:
- מספר שורות ריקות:13
- הצצה לערכי המטריצה: <img width="1109" height="134" alt="image" src="https://github.com/user-attachments/assets/8e652d97-7cc6-4ce1-a05d-c7cd3d1b337e" />


עבור קבצי המקור לאחר הניקוי בלי stopwords:
- מספר שורות ריקות:13
- הצצה לערכי המטריצה:<img width="1115" height="135" alt="image" src="https://github.com/user-attachments/assets/534e08f4-85de-4752-b7e4-cb0394fd151a" />
 

עבור קבצי הלמות עם stopwords: 
- מספר שורות ריקות:0
- הצצה לערכי המטריצה:<img width="1112" height="137" alt="image" src="https://github.com/user-attachments/assets/22311d22-341f-455a-a1bc-83e3f00ad84b" />
 

עבור קבצי הלמות בלי stopwords: 
- מספר שורות ריקות:0
- הצצה לערכי המטריצה: <img width="1129" height="140" alt="image" src="https://github.com/user-attachments/assets/75265c99-c597-4d1e-be6f-ff739b55b6d0" />
 

---
## שלב 6 – יצירת ייצוגים סמנטים למסמכים בעזרת SimCSE ו־SBERT
<div dir="rtl" align="right">
**מטרה:** יצירת ייצוגים סמנטיים למסמכי המקור באמצעות שני מודלים מתקדמים : 
1. SimCSE – Unsupervised Contrastive Learning <br>
מודל שלומד מה דומה ומה שונה בין משפטים לפי שלוקח אותו משפט פעמיים ומוסיף לו "רעש" ואז לומד לקרב בין 2 הגרסאות ויודע לייצר ייצוג טוב למשמעות של משפט או מסמך.
2. Sentence-BERT (SBERT)
מודל Transformer שמחזיר וקטור שמתאר את משמעות המשפט.
 
**ביצוע:**  
הרצת סקריפט (make_embeddings.py) שעובר על כל הקבצים בתיקייה `clean_text` שמכילה את הטקסט בלי תגי xml אבל לפני הפרדת סימני הפיסוק ומבצע את הצעדים הבאים:
1. טעינת המודלים: עבור `princeton-nlp/unsup-simcse-bert-base-uncased` -SimCSE 
               עבור `sentence-transformers/all-MiniLM-L6-v2` -SBERT  
2. שמירת רשימה של טקסטים ורשימה של שמות הקבצים, כדי לשייך בין הווקטורים שנוצרים למסמכים.
3. כל מסמך מקבל id ונעשה עליו חלוקה לצ'אנקים כי המודלים מקבלים עד 512 טוקנים, תהליך החלוקה :
   1. פיצול המסמך לפי מילים (מילה= רצף תווים עם רווח לפני ואחרי הרצף)
   2. לוקחים כל 200 מילים וממירים אותם לטוקנים ע"י הtokenizer שמתאים למודל sbert.
   3. אם הרצף שנוצר גדול מ512 מחלקים אותו לחלקים קטנים ומשייכים את כל החלקים לרשימה.
   4. הרשימה נשמרת משויכת בתוך מילון לid.

   
4. יצירת הוקטורים עבור כל הצ'אנקים ע"י כל אחד מהמודלים, זה נעשה בגודל batch 128 .
5. איתור הצ'אנקים של כל מסמך וחישוב ממוצע של הוקטורים של הצ'אנקים וזה וקטור המסמך.
6. שמירת מטריצת הSimCSE בפורמט `npz.` כי זו מטריצה דלילה.
7. שמירת מטריצת הSBERT בפורמט `npz.` כי זו מטריצה דלילה .
8. שמירת שמות הקבצים (= שורות המטריצות ) בפורמט JSON.
9. הרצת הסקריפט (check_matrix_st_sbert.py) בשביל לקבל קצת מידע על המטריצות שנוצרו- הצגת 10 המאפיינים הראשונים ו5 השורות הראשונות.

**תוצאות:**
התקבלה התיקייה embedding_out ובתוכה הקבצים:

- SimCSE-Origin.npz

וקטורים של כל המסמכים לפי מודל SimCSE.

- SBERT-Origin.npz

וקטורים של כל המסמכים לפי מודל SBERT.

- files_mapping.json

רשימה של שמות הקבצים לפי הסדר (כדי לדעת איזה וקטור שייך לאיזה מסמך לפי השורות של המטריצות) .

**ויזואליזציה של המטריצות שנוצרו:** <br> 
עבור המטריצה שנוצרה עם מודל SimCSE:
- מספר מאפיינים:768 
- הצצה לערכי המטריצה: <img width="1118" height="155" alt="image" src="https://github.com/user-attachments/assets/a0c9d5e5-db4a-4a0a-810f-1aa6690260ca" />

עבור המטריצה שנוצרה עם מודל SBERT:
- מספר מאפיינים:384 
- הצצה לערכי המטריצה: <img width="1120" height="152" alt="image" src="https://github.com/user-attachments/assets/2aa8e995-06c8-4fa8-92f4-8b259afae37e" />

</div>

---


## שלב 7 – חישוב Information Gain ו-Mutual Information
**מטרה:** להעריך את חשיבות כל מאפיין (מילה/למה) בהבחנה בין המסמכים .
המדד הנוסף שנבחר מלבד information gain הוא mutual information, הסיבה: <br>
המדד הזה מודד את התלות בין מאפיין לכל קטגוריה , כאן:
כל מסמך הוא קטגוריה בפני עצמו

וכל עמודה במטריצה = מילה או למה.
<br>
לכן MI נותן מדד מספרי כמה המילה/למות מסבירה את זהות המסמך .
המדד הזה בודק את התנהגות המילה מול המסמכים כך:
- אם המילה מופיעה רק במסמך אחד מסוים -> היא נותנת הרבה מידע על המסמך הזה -> MI גבוה.
- אם המילה מופיעה בכל המסמכים -> היא לא מבדלת בין מסמכים -> MI נמוך.
- אם המילה מופיעה בכמה מסמכים באופן חלקי -> היא נותנת קצת מידע -> MI בינוני.

החישוב בפועל:
- בודקים בכל מסמך אם המילה מופיעה או לא.
- משווים את הסיכוי שהמילה מופיעה בקטגוריה מסוימת מול הסיכוי הכללי שהמילה מופיעה ולסיכוי הכללי של המסמך.
- אם הסיכוי המשולב גדול יותר מהסיכוי שהיינו מצפים לו במקרה אקראי -> המילה חשובה יותר -> MI גבוה.
- אם הסיכוי המשולב לא שונה מהסיכוי האקראי -> המילה פחות משמעותית -> MI נמוך.

**ביצוע:**  
כתיבת סקריפט (measurments_calculation.py) שעובר על הקבצים בתיקייה `vectors_word` ופעם שנייה בתיקייה `vectors_lemm` , הסקריפט מבצע את הצעדים הבאים:
1. חישוב information gain באופן ידני: <br>
   1.1 טעינת מטריצת הtf-idf, שמות המסמכים, רשימת המאפיינים המתאימים לפי התיקייה שממנה לוקחים את הקבצים. <br>
   1.2 המרת המטריצה לפורמט numpy למחשוב נוח יותר. <br>
   1.3 המרת שמות הקבצים לקטגוריות (LabelEncoder). <br>
   1.4 מחשב Entropy של המסמך ולכל ערך של כל מאפיין. <br>
   1.5 מחשב Information Gain לפי ההפרש בין ה-Entropy הכללי לבין ה-Weighted Entropy של המאפיין. <br>
   1.6 יוצר DataFrame עם עמודות: שם המאפיין וערך המדד שחושב. <br>
2. חישוב mutual information:                             
   2.1 טעינת מטריצת הtf-idf, שמות המסמכים, רשימת המאפיינים המתאימים לפי התיקייה שממנה לוקחים את הקבצים. <br>
   2.2 המרת שמות הקבצים לקטגוריות (LabelEncoder). <br>
   2.3 מחשב Mutual Information ע"י הפונקציה `mutual_info_classif` מהספרייה  `sklearn` עבור כל מאפיין. <br>
   2.4 יוצר DataFrame עם עמודות: שם המאפיין וערך המדד שחושב. <br>
3. שמירה לקובץ אקסל טבלה עם העמודות : feature, information gain, mutual information (נעשה outer join על הdataframe שנוצרו כדי שהערכים יהיו תואמים למאפיין). 


**תוצאות:**  
התקבלו 2 קבצי אקסל- אחד לכל סוג מטריצה, במיקומים : vectors_word/tfidf_results.xlsx, vectors_lemm/tfidf_results.xlsx
 **20 המאפיינים החשובים לכל קבוצה**
- עבור TFIDF-Word: 
 <br><br> <img width="713" height="505" alt="image" src="https://github.com/user-attachments/assets/effdc899-fda6-4289-9734-374f8134d39b" /><br><br>
- עבור TFIDF-Lemm:
  <br><br><img width="916" height="507" alt="image" src="https://github.com/user-attachments/assets/07bb5daf-b533-4725-a508-71da7503e26d" />


## סיכום ותובנות

בתרגיל יצרנו תהליך מלא לעיבוד וייצוג של מסמכי הפרלמנט: הורדנו את קבצי ה-XML, ניקינו מהם תגיות ורעש, פירקנו למילים, יצרנו למות ובנינו ייצוגים שונים — החל מ-TF-IDF/BM25 ועד Word2Vec ו-SBERT/SimCSE. כל שלב סיפק סוג אחר של מידע על המסמך: שכיחות מילים, משמעות מילולית, ומשמעות עמוקה לפי מודלים מודרניים.

העבודה הראתה שבחירות שונות משפיעות על איכות הנתונים:  
למות מפחיתות מספר מאפיינים ושומרות על משמעות, הורדת מילים נדירות מנקה רעש, ו-BM25 מתאים במיוחד למסמכים קצרים. ב-Word2Vec ראינו שמסמכים דלים במילים מקבלים ייצוג חלש יותר, בעוד SBERT ו-SimCSE נותנים ייצוג יציב גם כשמבנה המשפטים משתנה.

בחישובי Information Gain ו-Mutual Information מצאנו אילו מילים ולמות מבדילות בין מסמכים. מילים שמופיעות בכל המסמכים קיבלו חשיבות נמוכה, ומילים שמופיעות רק בחלק קטן מהמסמכים קיבלו חשיבות גבוהה. כך ניתן לזהות מאפיינים מרכזיים ולשפר את הבנת התוכן.

בסך הכול, התרגיל הדגים איך מעבר בין שלבים שונים — מניקוי בסיסי ועד מודלים מתקדמים — בונה תמונה מלאה ומדויקת של כל מסמך בקורפוס.

