📘 שלבי העבודה בפרויקט – קורס איחזור מידע
🟦 שלב 1 – הורדת קבצי ה־XML

שם הסקריפט: download_debates.py

🎯 מטרה

להוריד את כל קבצי ה־XML של פרוטוקולי הפרלמנט הבריטי החל מהקובץ debates2023-06-28d.xml ועד הסוף (כ־935 קבצים) לצורך עיבוד בהמשך.

⚙️ מה נעשה

נכתב סקריפט בפייתון המשתמש בספריות requests ו־tqdm להורדה אוטומטית של הקבצים.

הסקריפט:

שואב את רשימת הקבצים מהאתר.

מסנן לפי שם הקובץ המבוקש.

מוריד כל קובץ לתיקייה debates_xml תוך הצגת התקדמות.

מדלג על קבצים שכבר הורדו.

💡 הסיבה לבחירה

שיטה פשוטה, אמינה וחסכונית — מאפשרת הורדה אוטומטית מלאה ללא שימוש ידני בדפדפן, עם הגנות בפני ניתוקים או שגיאות רשת.

📈 תוצאות

כל הקבצים ירדו בהצלחה לתיקייה מקומית.

שמות הקבצים נשמרו במדויק.

שגיאות אפשריות:

ניתוק רשת או Timeout → נפתר באמצעות מנגנון Retry.

קובץ פגום → נבדק לפי גודל, והסקריפט מדלג ומנסה שוב.

🟩 שלב 2 – ניקוי קבצי ה־XML ובדיקת קבצים ריקים

שמות הסקריפטים:

clean_xml_folder.py – ניקוי תגיות XML.

check_empty_files.py – בדיקת קבצים ריקים.

🎯 מטרה

להמיר את קבצי ה־XML לקבצי טקסט נקיים המכילים רק את תוכן הדיבור (ללא תגיות HTML או XML), ולוודא שכל הקבצים שנוצרו אכן תקינים ולא ריקים.

⚙️ מה בוצע
1️⃣ ניקוי תגיות ה־XML

שימוש בספרייה lxml לעיבוד כל קובץ XML.

חילוץ כל הטקסט שבין התגים באמצעות itertext().

הסרת רווחים ותווים מיותרים.

שמירת קובצי טקסט חדשים (.txt) בתיקייה clean_text בשם זהה למקור.

טיפול בשגיאות פרסינג באמצעות קובץ לוג error_log.txt.

2️⃣ בדיקת קבצים ריקים

מעבר על כל קובצי הטקסט בתיקייה.

בדיקה אם גודל הקובץ הוא 0 או שהתוכן קצר מ־10 תווים.

הדפסת רשימת כל הקבצים הריקים או כמעט ריקים.

💡 הסיבה לבחירה

lxml מאפשרת ניתוח מדויק של מבנה XML גם בקבצים מורכבים.

בדיקת קבצים ריקים מבטיחה שקבצים ריקים לא ישפיעו על תוצאות ניתוח המידע בהמשך (TF-IDF, Embeddings).

📈 תוצאות

נוצרו קבצי טקסט נקיים בתיקייה clean_text.

נוצר קובץ לוג לשגיאות פרסינג.

הופקה רשימת קבצים ריקים שנמצאו:

נמצאו 13 קבצים ריקים או כמעט ריקים:
- debates2023-11-14b.txt
- debates2023-12-13b.txt
- debates2023-12-18b.txt
- debates2024-02-06c.txt
- debates2024-05-23b.txt
- debates2024-05-24b.txt
- debates2024-07-17e.txt
- debates2024-12-12b.txt
- debates2025-01-17c.txt
- debates2025-01-22a.txt
- debates2025-03-20a.txt
- debates2025-07-21c.txt
- debates2025-07-22c.txt

🟨 שלב 3 – ניקוי טקסט והפרדת מילים מסימני פיסוק

שם הסקריפט: tokenize_clean_text.py

🎯 מטרה

להבטיח שכל מילה תעמוד בפני עצמה ללא סימני פיסוק צמודים, כך שהמערכת תוכל לבצע חישובי TF-IDF ו־Embeddings בצורה מדויקת.

⚙️ מה בוצע

נכתב סקריפט המשתמש ב־Regex לטוקניזציה מתקדמת.

מזהה מילים בעברית ובאנגלית, כולל תווים מיוחדים כמו גרשיים עבריים (צה"ל, צה״ל).

מפריד סימני פיסוק (. , : ; ? ! ( ) [ ] " - …) כך שכל אחד נשמר כטוקן עצמאי.

שומר את התוצאה בתיקייה tokens תוך שמירה על שמות הקבצים המקוריים.

מטפל בשגיאות קריאה וכתיבה, ומדלג על קבצים בעייתיים.

💡 הסיבה לשיטה

גישה זו שומרת על מבנה המשפט אך מאפשרת ניתוח מילים נפרדות לצורך ניתוח לשוני מדויק, הפחתת רעש ושיפור איכות האינדקס (למשל: ממשלה. לא תיחשב שונה מ־ממשלה).

📈 תוצאות

נוצרו גרסאות “מטוקננות” של כל הקבצים בתיקייה tokens.

כל מילה וסימן פיסוק מופרדים ברווח אחד בלבד.

הנתונים מוכנים כעת לשלב הבא – הסרת stopwords ולממטיזציה (lemmatization).

בשלב יצירת גרסאות הלמות בחרנו להשתמש בכלי spaCy, ובפרט במודל en_core_web_sm.
הספרייה spaCy נבחרה בזכות הדיוק הגבוה שלה בלמטיזציה מבוססת הקשר תחבירי (POS tagging) ובזכות ביצועיה הטובים בעיבוד טקסטים ארוכים באנגלית.
הכלי יודע לזהות את הצורה הבסיסית (lemma) של כל מילה בהתאם להקשרה במשפט, מה שמפחית וריאציות מיותרות ומאפשר ייצוגים עקביים יותר בשלבים הבאים של חישוב TF-IDF ו־Word2Vec.
התהליך בוצע על כל קובצי הטקסט הנקיים מתגי XML, כאשר לכל קובץ נוצר קובץ חדש בשם זהה בתיקייה נפרדת lemmatized_text.

🧩 סיכום ביניים – עד שלב 4.1
🎯 מטרה כללית

להכין את הנתונים הגולמיים (פרוטוקולים מהפרלמנט הבריטי) כך שכל מסמך יוצג בצורה מספרית מדויקת — כדי שנוכל בהמשך להשוות, לנתח ולחשב רלוונטיות בין מסמכים.

🔹 מה בוצע בפועל

הורדת הקבצים – הורדנו מאות קבצי XML מהמאגר הרשמי של הפרלמנט הבריטי.

ניקוי התוכן – הסרנו תגיות XML, תווים מיוחדים וסימני פיסוק, והפקנו טקסט נקי.

הפרדת מילים (Tokenization) – דאגנו שכל מילה וסימן פיסוק יעמדו בנפרד.

יצירת למות (Lemmatization) – הפכנו מילים לצורת השורש שלהן (כמו running → run).

בניית מטריצות TF-IDF ו־BM25 (Word-level) –
לכל מסמך יצרנו וקטור מספרי שמראה אילו מילים מופיעות בו וכמה הן חשובות,
תוך שימוש בצמצום מאפיינים (הסרת stopwords ומילים נדירות).

📊 תוצאות

957 מסמכים

29,162 מאפיינים (מילים שונות)

צפיפות מטריצה: כ־7.5% (מטריצה דלילה — רוב הערכים אפסיים)

נשמרו:

TFIDF_Word.npz

BM25_Word.npz

מיפוי מילים (vocabulary.json)

מיפוי שמות קבצים (files.json)

💡 משמעות התוצאה

עכשיו כל מסמך מוכן לשלב הבא —
למדוד חשיבות מילים בצורה מתקדמת יותר (על בסיס למות)
ולעבור למודלים סמנטיים כמו Word2Vec, GloVe ו־SBERT.

תוצאה: (venv) C:\Users\user\Desktop\שנה ד\איחזור מידע\ir>python build_vectors_word.py --input lemmatized_text --outdir vectors_lemm
נקראו 943 מסמכים מ-lemmatized_text
TF-IDF shape: (943, 22993), צפיפות: 0.078101
Counts shape: (943, 22993)

✅ נשמרו קבצים בתיקייה: C:\Users\user\Desktop\שנה ד\איחזור מידע\ir\vectors_lemm
  - TFIDF_Word.npz + vocab + files
  - BM25_Word.npz  + vocab + files



🧩 סיכום – שלב 4.1 (Lemmas)
🎯 מטרה

לבנות מטריצות TF-IDF ו-BM25 על בסיס הלמות (צורות השורש של המילים),
כדי לבדוק האם ייצוג לפי למות מפחית רעש לשוני ומשפר את איכות המידע לעומת ייצוג לפי מילים רגילות.

⚙️ מה בוצע

נקראו 943 מסמכים מתוך התיקייה lemmatized_text.

נוצרו שתי מטריצות:

TF-IDF_Lemm — חישוב חשיבות של כל למָה בכל מסמך.

BM25_Lemm — גרסה משוקללת עם התאמה לאורך המסמך.

הוסרו stop-words באנגלית, ומילים נדירות מדי (פחות מ-5 הופעות).

נשמרו גם מיפויים (vocabulary.json, files.json) כדי לדעת איזו עמודה שייכת לאיזו למָה ואיזו שורה לאיזה קובץ.

📊 תוצאות

מספר מסמכים: 943

מספר למות ייחודיות: 22,993

צפיפות: ‎≈ 0.078 (דלילה מאוד – כמו שצפוי בטקסט טבעי)

תיקיית פלט: vectors_lemm/

TFIDF_Lemm.npz

BM25_Lemm.npz

קבצי מיפוי נלווים (*.json)

💡 תובנה

בהשוואה לגרסת המילים (Word-level), מתקבלת מטריצה קטנה יותר
(פחות מאפיינים – 22.9K לעומת 29.1K), כי צורות שונות של אותה מילה אוחדו.
זה מאשר שהלמטיזציה הפחיתה כפילויות ואחידות את השפה, מה שמייעל את ניתוח הנתונים בהמשך.

התוצאה: 
נקראו 943 מסמכים מ-lemmatized_text TF-IDF shape: (943, 22993), צפיפות: 0.078101 Counts shape: (943, 22993) ✅ נשמרו קבצים בתיקייה: C:\Users\user\Desktop\שנה ד\איחזור מידע\ir\vectors_lemm - TFIDF_Word.npz + vocab + files - BM25_Word.npz + vocab + files


🧩 סיכום – שלב 4.2 (Word2Vec)
🎯 מטרה

לייצר ייצוג סמנטי לכל מסמך בעזרת Word2Vec, כך שמילים בעלות משמעות דומה יקבלו ייצוגים קרובים במרחב המספרי.

⚙️ מה בוצע

אימון שני מודלי Word2Vec – אחד על מילים רגילות (tokens) ואחד על למות (lemmas).

לכל מסמך חושב וקטור ממוצע של מילותיו (ממד 300).

נבנו שתי גרסאות לכל רמה:

basic – ללא פיסוק, מספרים ותווים מיוחדים.

nostop – בנוסף גם ללא stop-words.

📊 תוצאות

Word-level: ‎957 מסמכים × 300 ממדים.

Lemma-level: ‎943 מסמכים × 300 ממדים.

נשמרו בקבצים בתיקייה embeddings_w2v/:

W2V_Word_basic.npy, W2V_Word_nostop.npy

W2V_Lemm_basic.npy, W2V_Lemm_nostop.npy

ו־w2v_model_word.model, w2v_model_lemm.model