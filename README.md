📘 שלבי העבודה בפרויקט – קורס איחזור מידע
🟦 שלב 1 – הורדת קבצי ה־XML

שם הסקריפט: download_debates.py

🎯 מטרה

להוריד את כל קבצי ה־XML של פרוטוקולי הפרלמנט הבריטי החל מהקובץ debates2023-06-28d.xml ועד הסוף (כ־935 קבצים) לצורך עיבוד בהמשך.

⚙️ מה נעשה

נכתב סקריפט בפייתון המשתמש בספריות requests ו־tqdm להורדה אוטומטית של הקבצים.

הסקריפט:

שואב את רשימת הקבצים מהאתר.

מסנן לפי שם הקובץ המבוקש.

מוריד כל קובץ לתיקייה debates_xml תוך הצגת התקדמות.

מדלג על קבצים שכבר הורדו.

💡 הסיבה לבחירה

שיטה פשוטה, אמינה וחסכונית — מאפשרת הורדה אוטומטית מלאה ללא שימוש ידני בדפדפן, עם הגנות בפני ניתוקים או שגיאות רשת.

📈 תוצאות

כל הקבצים ירדו בהצלחה לתיקייה מקומית.

שמות הקבצים נשמרו במדויק.

שגיאות אפשריות:

ניתוק רשת או Timeout → נפתר באמצעות מנגנון Retry.

קובץ פגום → נבדק לפי גודל, והסקריפט מדלג ומנסה שוב.

🟩 שלב 2 – ניקוי קבצי ה־XML ובדיקת קבצים ריקים

שמות הסקריפטים:

clean_xml_folder.py – ניקוי תגיות XML.

check_empty_files.py – בדיקת קבצים ריקים.

🎯 מטרה

להמיר את קבצי ה־XML לקבצי טקסט נקיים המכילים רק את תוכן הדיבור (ללא תגיות HTML או XML), ולוודא שכל הקבצים שנוצרו אכן תקינים ולא ריקים.

⚙️ מה בוצע
1️⃣ ניקוי תגיות ה־XML

שימוש בספרייה lxml לעיבוד כל קובץ XML.

חילוץ כל הטקסט שבין התגים באמצעות itertext().

הסרת רווחים ותווים מיותרים.

שמירת קובצי טקסט חדשים (.txt) בתיקייה clean_text בשם זהה למקור.

טיפול בשגיאות פרסינג באמצעות קובץ לוג error_log.txt.

2️⃣ בדיקת קבצים ריקים

מעבר על כל קובצי הטקסט בתיקייה.

בדיקה אם גודל הקובץ הוא 0 או שהתוכן קצר מ־10 תווים.

הדפסת רשימת כל הקבצים הריקים או כמעט ריקים.

💡 הסיבה לבחירה

lxml מאפשרת ניתוח מדויק של מבנה XML גם בקבצים מורכבים.

בדיקת קבצים ריקים מבטיחה שקבצים ריקים לא ישפיעו על תוצאות ניתוח המידע בהמשך (TF-IDF, Embeddings).

📈 תוצאות

נוצרו קבצי טקסט נקיים בתיקייה clean_text.

נוצר קובץ לוג לשגיאות פרסינג.

הופקה רשימת קבצים ריקים שנמצאו:

נמצאו 13 קבצים ריקים או כמעט ריקים:
- debates2023-11-14b.txt
- debates2023-12-13b.txt
- debates2023-12-18b.txt
- debates2024-02-06c.txt
- debates2024-05-23b.txt
- debates2024-05-24b.txt
- debates2024-07-17e.txt
- debates2024-12-12b.txt
- debates2025-01-17c.txt
- debates2025-01-22a.txt
- debates2025-03-20a.txt
- debates2025-07-21c.txt
- debates2025-07-22c.txt

🟨 שלב 3 – ניקוי טקסט והפרדת מילים מסימני פיסוק

שם הסקריפט: tokenize_clean_text.py

🎯 מטרה

להבטיח שכל מילה תעמוד בפני עצמה ללא סימני פיסוק צמודים, כך שהמערכת תוכל לבצע חישובי TF-IDF ו־Embeddings בצורה מדויקת.

⚙️ מה בוצע

נכתב סקריפט המשתמש ב־Regex לטוקניזציה מתקדמת.

מזהה מילים בעברית ובאנגלית, כולל תווים מיוחדים כמו גרשיים עבריים (צה"ל, צה״ל).

מפריד סימני פיסוק (. , : ; ? ! ( ) [ ] " - …) כך שכל אחד נשמר כטוקן עצמאי.

שומר את התוצאה בתיקייה tokens תוך שמירה על שמות הקבצים המקוריים.

מטפל בשגיאות קריאה וכתיבה, ומדלג על קבצים בעייתיים.

💡 הסיבה לשיטה

גישה זו שומרת על מבנה המשפט אך מאפשרת ניתוח מילים נפרדות לצורך ניתוח לשוני מדויק, הפחתת רעש ושיפור איכות האינדקס (למשל: ממשלה. לא תיחשב שונה מ־ממשלה).

📈 תוצאות

נוצרו גרסאות “מטוקננות” של כל הקבצים בתיקייה tokens.

כל מילה וסימן פיסוק מופרדים ברווח אחד בלבד.

הנתונים מוכנים כעת לשלב הבא – הסרת stopwords ולממטיזציה (lemmatization).

בשלב יצירת גרסאות הלמות בחרנו להשתמש בכלי spaCy, ובפרט במודל en_core_web_sm.
הספרייה spaCy נבחרה בזכות הדיוק הגבוה שלה בלמטיזציה מבוססת הקשר תחבירי (POS tagging) ובזכות ביצועיה הטובים בעיבוד טקסטים ארוכים באנגלית.
הכלי יודע לזהות את הצורה הבסיסית (lemma) של כל מילה בהתאם להקשרה במשפט, מה שמפחית וריאציות מיותרות ומאפשר ייצוגים עקביים יותר בשלבים הבאים של חישוב TF-IDF ו־Word2Vec.
התהליך בוצע על כל קובצי הטקסט הנקיים מתגי XML, כאשר לכל קובץ נוצר קובץ חדש בשם זהה בתיקייה נפרדת lemmatized_text.

