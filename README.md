
### תיעוד לחישוב מדדים עבור מטריצת TFIDF (Information Gain ו-Mutual Information)

#### 1. **הקדמה:**

בתרגיל זה, המטרה היא לחשב את חשיבות המאפיינים (מילים או למות) במטריצות TFIDF שונות, על ידי שימוש בשני מדדים: **Information Gain** ו-**Mutual Information** (המצוין בשם שיטה שניה במטלה). התוצאה הסופית תהיה קובץ Excel הכולל את חשיבות המאפיינים עבור כל אחד מהמדדים.

#### 2. **מדדים:**

* **Information Gain**:

  * **תיאור**: Information Gain (IG) מודד את כמות המידע שניתן להשיג ממאפיין מסוים על ידי חישוב השינוי בהסתברות של היעד כאשר ידוע המאפיין.
  * **השימוש כאן**: החישוב מבוצע על פי המטריצה של TFIDF, כשהמטרה היא לחשב את ה-IG של כל אחד מהמאפיינים במטריצה ביחס לקטגוריות של הקבצים (כגון נושאים או תגיות).
  * **תהליך חישוב**: תחילה, חושבים את ה-Entropy הכללי של הקטגוריות (תוויות). לאחר מכן, עבור כל מאפיין במטריצה (מילה או למה), מבוצע חישוב ה-Entropy של קבוצות המשויכות לכל ערך של המאפיין. ה-Information Gain מחושב כהפרש בין ה-Entropy הכללי לבין סכום ה-Entropy של כל הקבוצות.

* **Mutual Information**:

  * **תיאור**: Mutual Information (MI) מודד את כמות המידע המשותף בין שני משתנים, במקרה שלנו בין המאפיינים של מטריצת TFIDF לבין התוויות של הקבצים. MI מבוסס על חישוב ההסתברויות המשותפות של המאפיינים והקטגוריות, ומודד עד כמה המידע של מאפיין מסוים תורם להבדלה בין הקטגוריות השונות.
  * **השימוש כאן**: Mutual Information נבחר כמדד נוסף כדי להוסיף עוד פרספקטיבה על חשיבות המאפיינים, זאת משום שהוא מודד את הקשר הסיבתי בין המאפיינים והיעד (קטגוריות). הוא מתאים במיוחד במקרים בהם יש צורך להעריך את עוצמת הקשר בין משתנים שונים, כמו כאן, בו נרצה להבין עד כמה כל מילה או למה במטריצה תורמת להבחנה בין הקטגוריות השונות.

#### 3. **הסיבה לבחירה ב-Mutual Information כמדד נוסף:**

מבין המדדים השונים המוצעים (Gain Ratio, Gini Impurity, Chi-squared statistic, ועוד), ה-**Mutual Information** נבחר מכיוון שהוא:

* **יעיל בהערכת הקשר בין משתנים קטגוריאליים**: כפי שצוין, מטרת החישוב היא להבין את חשיבות המאפיינים (מילים או למות) בהקשר של הקטגוריות. Mutual Information מצוין למקרים של משתנים קטגוריאליים, שכן הוא מודד את הקשר המשותף בין המאפיינים לבין התוויות. לעומת זאת, מדדים כמו **Gini Impurity** או **Gain Ratio** מיועדים בעיקר למודלים של עץ החלטה ואינם בהכרח מייצגים את הקשר בין מאפיינים (כמו מילים) לבין תוויות.
* **הבנה טובה של חפיפות מידע**: ה-Mutual Information לא רק בודק את הקשר, אלא גם מודד את כמות המידע המשותף. זה מאפשר לנו להבין אילו מילים או למות תורמות ביותר להבחנה בין הקטגוריות השונות. מדדים אחרים, כמו **Chi-squared statistic**, אמנם בודקים אם יש קשר בין משתנים, אך אינם מספקים אינדיקציה ישירה לכמות המידע המשותף ביניהם.
* **פשטות ויעילות בחישוב**: Mutual Information הוא יחסית פשוט וקל לשימוש, במיוחד עם ספריות קיימות כמו `mutual_info_classif` בסקikit-learn, מה שמקל על החישוב במטריצות גדולות של TFIDF.

#### 4. **תהליך החישוב:**

* **Information Gain**:

  * המטריצה נטענת כקובץ `.npz` (Sparse Matrix).
  * המילים והמאפיינים נטענים מקובץ `vocabulary.json`.
  * התוויות (קטגוריות) נטענות מקובץ `files.json`.
  * חישוב ה-Information Gain מתבצע לכל מאפיין במטריצה, תוך חישוב ה-Entropy הכללי של התוויות וה-Entropy לכל ערך במאפיין.
* **Mutual Information**:

  * המטריצה נטענת כקובץ `.npz`.
  * התוויות נטענות מקובץ `files.json`.
  * השיטה `mutual_info_classif` מחסיקה את חישוב ה-Mutual Information לכל מאפיין ביחס לתוויות. היא מבוססת על הסתברויות ומספקת את כמות המידע המשותף.

#### 5. **שמירת התוצאות**:

לאחר חישוב שני המדדים (Information Gain ו-Mutual Information), התוצאות נשמרות לקובץ Excel בעזרת הפונקציה `save_to_excel`. כל שורה בקובץ תכיל את המאפיין (מילה או למה), ואת ערך ה-Information Gain ו-Mutual Information עבורו.

#### 6. **סיכום**:

השימוש ב-Information Gain וב-Mutual Information מספק מבט מקיף על החשיבות של כל מאפיין במטריצת TFIDF ומאפשר לנו לבחור את המילים או הלמות החשובות ביותר בהבחנה בין הקטגוריות השונות. הבחירה ב-Mutual Information נועדה להוסיף מימד נוסף של חפיפות מידע ומספקת כלי נוסף להעריך את תרומת המאפיינים למודל האחזור.


דוח מעודכן ⬇️

# דוח עבודה – קורס אחזור מידע / תרגיל 1
תיעוד מלא של שלבי עיבוד הנתונים, ההכנה ובניית הייצוגים למסמכי הפרלמנט הבריטי.

---
## שלב 1 – הורדת קבצי XML
**מטרה:** לאסוף את כל מסמכי המקור הגולמיים (XML) כדי לעבד אותם בהמשך.

**ביצוע:**  
נכתב סקריפט (download_debates.py) המבצע הורדה אוטומטית של כל קובץ XML מהאתר: https://www.theyworkforyou.com/pwdata/scrapedxml/debates/ החל מהקובץ debates2023-06-28d.xml ועד סוף הארכיון. <br> 
הסקריפט שולף את רשימת הקבצים מהאתר הרשמי, מסנן לפי תבנית שם, ומוריד כל מסמך לתיקייה מסודרת. 

**תוצאות:**  
התקבלה ספרייה בשם debates_xml שמכילה 956 קבצים.

---

## שלב 2 – ניקוי קבצי הXML
### שלב 2.1 – ניקוי תגיות XML
**מטרה:** להפיק טקסט נקי מתגיות ולהבטיח שרק קבצים בעלי תוכן יעברו לעיבוד.

**ביצוע:**  
נכתב סקריפט (clean_tags.py) שמעבד את קבצי הxml בעזרת הספרייה `lxml` ומשאיר רק את הטקסט שהיה בין התגיות. 
לאחר השלמת פעולת הניקוי הורץ קוד שבודק כמה קבצים ריקים יש - אין בהן תוכן, ואת שמותיהם ואותם מחקנו כי לא נותנים שום מידע.  

**תוצאות:**  
התקבלה ספרייה בשם clean_text שמכילה את הקבצים הנקיים מתגים בלי 13 קבצים הריקים ששמותיהם:

 debates2023-11-14b.txt <br>
 debates2023-12-13b.txt <br>
 debates2023-12-18b.txt <br>
 debates2024-02-06c.txt <br>
 debates2024-05-23b.txt <br>
 debates2024-05-24b.txt <br>
 debates2024-07-17e.txt <br>
 debates2024-12-12b.txt <br>
 debates2025-01-17c.txt <br>
 debates2025-01-22a.txt <br>
 debates2025-03-20a.txt <br>
 debates2025-07-21c.txt <br>
 debates2025-07-22c.txt <br>

 סה"כ יש 943 קבצים בתיקייה לעיבוד עתידי.


### שלב 2.2 – טוקניזציה (Tokenization)
**מטרה:** לפרק את הטקסט למילים וסימני פיסוק נפרדים, כדי של מילה תהיה משמעות בפני עצמה ללא סימני פיסוק צמודים כך שיהיה אפשר לבצע חישובים בצורה מדוייקת.

**ביצוע:**  
נכתב סקריפט (tokenize_clean_text.py) שעובר על כל אחד מהקבצים שנוצרו בשלב 2.1 ומשתמש במנגנון המובנה של פייתון str.translate וב־string.punctuation כדי לזהות את כל סימני הפיסוק הנפוצים: !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~<br> 
לכל סימן פיסוק מוגדר תרגום המכניס רווח לפני ואחרי הסימן.
כך, בעת העיבוד, כל סימני הפיסוק מופרדים אוטומטית מהמילים.

**תוצאות:**  
התקבלה ספרייה בשם tokens שמכילה את הטקסט עם סימני פיסוק מופרדים.<br> 
הערה: מילים עם s השייכות שמופיעות כך: s' לא הופרדו כי זה חלק מהמילה ומציין שייכות ולא כמו סימני פיסוק אחרים שהם חלק מהמשפט ולא מהמילה. 


---

## שלב 3 – Lemmatization
**מטרה:** לצמצם וריאציות לשוניות על ידי המרה לצורות בסיס (lemmas), ולהקטין את מספר המאפיינים.

**ביצוע:**
כתיבת סקריפט (lemma.py) שעובר על כל הקבצים בתיקייה tokens ומשתמש בכלי spaCy עם המודל en_core_web_sm שמזהה את תפקיד המילה במשפט (POS) וכך עוזר ליצור את הלמה של המילה.

**תוצאות:**
התקבלה תיקייה lemmatized_text שהמילים בכל קובץ מופיעות בצורת הלמה שלהן.

בחרנו להשתמש ב-spaCy בשביל ליצור למות כי כי הוא יודע להחזיר את צורת השורש הנכונה של מילה לפי ההקשר במשפט,— הוא מזהה את חלקי המשפט ואת התפקיד של כל מילה ומחזיר צורה בסיסית נקייה שמתאימה לעיבוד נוסף. בנוסף הוא מהיר מאוד, יציב, ויכול לעבור על מאות מסמכים בלי בעיות. כלים אחרים כמו WordNet או TextBlob עובדים לפי מילונים ישנים ומחזירים לפעמים צורות לא מדויקות, ו-Snowball או Porter פשוט “חותכים” מילים ויוצרים מילים לא אמיתיות. Stanza גם מספק תוצאות טובות, אבל הוא כבד ואיטי בהרבה. לכן spaCy נותן את השילוב הנכון של איכות וביצועים, שמתאים לצינור העבודה שלנו.

---

## שלב 4 - בניית וקטורים לייצוג TF-IDF עם BM25\Okapi

**מטרה:**  
יצירת וקטור מאפיינים לכל מסמך. כל מסמך ממופה לווקטור מספרי המתאר את המילים שמופיעות בו ואת החשיבות היחסית שלהן באמצעות TF-IDF עם BM25\Okapi.

**ביצוע:**  
כתיבת סקריפט (`build_vectors_word.py`) שעובר על כל הקבצים פעמיים: פעם אחת בתיקייה `tokens` ופעם שנייה בתיקייה `lemmatized_text`. הסקריפט מבצע את הצעדים הבאים:

1. שמירת רשימה של טקסטים ורשימה של שמות הקבצים, כדי לשייך בין הווקטורים שנוצרים למסמכים.
2. המרת הטקסטים למטריצות שבהן כל תא מכיל את כמות הפעמים שהמילה מופיעה במסמך (TF) באמצעות הפונקציה `CountVectorizer` מתוך `scikit-learn`:

```python
from sklearn.feature_extraction.text import CountVectorizer

count_vec = CountVectorizer(
    input="content",
    analyzer="word",
    stop_words="english",
    min_df=5
)
```
| פרמטר                  | משמעות                             |
| ---------------------- | ---------------------------------- |
| `input="content"`      | מציין שהקלט הוא טקסט גולמי         |
| `analyzer="word"`      | מפצל את הטקסט למילים               |
| `stop_words="english"` | מסיר stopwords באנגלית             |
| `min_df=5`             | מסיר מילים שמופיעות פחות מ-5 פעמים |

3. חישוב כל תא במטריצה באמצעות BM25\Okapi ידנית עם הפרמטרים `k=1.5` ו-`b=0.75`.  
4. שמירת המטריצה שנוצרה בפורמט `.npz` כדי לשמור מטריצה דלילה.  
5. שמירת המילים עצמם בפורמט JSON, כאשר המפתח הוא המילה והערך הוא מספר הפעמים שהיא מופיעה.  
6. שמירת שמות הקבצים בפורמט JSON.
7. הרצת סקריפט check_matrix.py בשביל לקבל קצת מידע על המטריצה שנוצרה - כמה מאפיינים יש , כמה שורות ריקות (אם ישנן) , הצגת 10 המאפיינים עם סכום העמודה הגבוה ביותר ו5 השורות הראשונות.

בחרנו לכתוב את הקוד בצורה הזאת כי זה הפתרון הכי מסודר ויעיל , השתמשנו בכלים מוכנים של scikit-learn שמייצרים את המטריצות בצורה אוטומטית ומהירה, ובנינו פונקציות כלליות שאפשר להריץ גם על מילים רגילות וגם על למות בלי לשכפל קוד. בנוסף שמרנו את המטריצות במבנה דליל (sparse) כדי לחסוך מקום בזיכרון, כי רוב הערכים הם אפסים. מימוש BM25 נעשה ידנית כדי שתהיה לנו שליטה על הפרמטרים וכדי להבין באמת איך המשקל מחושב. בסוף זה נותן קוד ברור, יעיל וקל לתחזוקה.
---

**תוצאות:**  
לאחר שתי הרצות התקבלו שתי תיקיות: `vectors_word` ו-`vectors_lemm`, שבהן יש את הקבצים הבאים:

- **מטריצת TF-IDF:**  
   `TFIDF-Word.npz` / `TFIDF-Lemm.npz`

- **המילים בעמודות המטריצה:**  
   `TFIDF-Word_vocabulary.json` / `TFIDF-Lemm_vocabulary.json`

- **שמות המסמכים (השורות במטריצה):**  
   `TFIDF-Word_files.json` / `TFIDF-Lemm_files.json`

**ויזואליזציה של המטריצות שנוצרו:** <br> 
עבור קבצי המקור לאחר הניקוי :
- מספר המאפיינים: 29151
- מספר שורות ריקות:0
- הצצה לערכי המטריצה: <img width="966" height="160" alt="image" src="https://github.com/user-attachments/assets/ff14b0ab-a480-4366-beb8-ab1e61850848" />
**ויזואליזציה של המטריצות שנוצרו:** <br> 
עבור קבצי הלמות :
- מספר המאפיינים: 22993
- מספר שורות ריקות:0
- הצצה לערכי המטריצה: <img width="916" height="186" alt="image" src="https://github.com/user-attachments/assets/423a5454-3340-4eeb-99a9-d87c588fcee0" />


---

## שלב 5 – בניית ייצוגים סמנטיים (Word2Vec)
**מטרה:** לייצג מסמכים לפי המשמעות, לא רק לפי שכיחות מילים.

**ביצוע:**
כתיבת סקריפט (build_doc_embeddings_w2v.py) שעובר על כל הקבצים פעמיים: פעם אחת בתיקייה `tokens` ופעם שנייה בתיקייה `lemmatized_text`. הסקריפט מבצע את הצעדים הבאים:

1. שמירת רשימה של טקסטים ורשימה של שמות הקבצים, כדי לשייך בין הווקטורים שנוצרים למסמכים.
2. פיצול הטקסט למילים וניקוי פעמיים: בפעם הראשונה- הוצאת סימני פיסוק, סינון מילים שמכילים ספרות, סינון טוקנים קצרים חסרי משמעות (למעט "i", "a" ) ובפעם השנייה כמו בפעם הראשונה אבל גם עם סינון stopwords באנגלית.
3. אימון המודל:
   1. הגדרות- גודל הוקטור: 300, גודל החלון: 5 , מתעלמים ממילים שמופיעות פחות מ5 פעמים, מבצעים 10 epochs.
   2. ``` model.build_vocab(sentences) ``` בניית אוצר מילים מתוך כל המילים המופיעות מספיק פעמים.
   3. אימון המודל - למידת embeddings לכל מילה בהתאם להקשר שלה (CBOW/Skip-gram).
      (המודל שנוצר מכיל וקטור צפוף לכל מילה שזוהתה.)
4.  יצירת וקטורי מסמך: לכל מסמך בודקים איזה מילים קיימות במודל הword2vec שאומן ולכל מילה שנמצאה לוקחים את הוקטור שלה וסוכמים עם שאר המילים שנמצאו ועושים ממוצע והתוצאה זה וקטור המסמך . <br>
הערה: אם המסמך לא מכיל מילים אחרי הסינון הוקטור שלו הוא וקטור ה0. 
5. שמירת המטריצה שנוצרה בפורמט `.npy` כדי לשמור מטריצה צפופה -> נעשה פעמיים : פעם אחת עם stopwords ופעם שנייה בלי stopwords. 
6. שמירת שמות הקבצים (= שורות המטריצה ) בפורמט JSON -> נעשה פעמיים : פעם אחת עם stopwords ופעם שנייה בלי stopwords.
7. הרצת סקריפט (check_matrix_w2v.py) בשביל לקבל קצת מידע על המטריצה שנוצרה - כמה שורות ריקות (אם ישנן) , הצגת 10 המאפיינים הראשונים מתוך 300 מאפיינים ו5 השורות הראשונות. 
   
**תוצאות:**
התקבלו הקבצים הבאים:
| קובץ / אובייקט         | תוכן                                          |
| ---------------------- | --------------------------------------------- |
| `W2V_Word_basic.npy`   | וקטורי מסמכים Word-level ללא הסרת stop-words  |
| `W2V_Word_nostop.npy`  | וקטורי מסמכים Word-level עם הסרת stop-words   |
| `W2V_Lemm_basic.npy`   | וקטורי מסמכים Lemma-level ללא הסרת stop-words |
| `W2V_Lemm_nostop.npy`  | וקטורי מסמכים Lemma-level עם הסרת stop-words  |
| `W2V_Word_files.json`     | שמות הקבצים ושיוך לשורות במטריצה              |
| `W2V_Lemm_files.json`     | שמות הקבצים ושיוך לשורות במטריצה              |
| `w2v_model_word.model` | מודל Word2Vec מאומן על Word-level             |
| `w2v_model_lemm.model` | מודל Word2Vec מאומן על Lemma-level            |

**ויזואליזציה של המטריצות שנוצרו:** <br> 
עבור קבצי המקור לאחר הניקוי עם stopwords:
- מספר שורות ריקות:13
- הצצה לערכי המטריצה: <img width="1109" height="134" alt="image" src="https://github.com/user-attachments/assets/8e652d97-7cc6-4ce1-a05d-c7cd3d1b337e" />


עבור קבצי המקור לאחר הניקוי בלי stopwords:
- מספר שורות ריקות:13
- הצצה לערכי המטריצה:<img width="1115" height="135" alt="image" src="https://github.com/user-attachments/assets/534e08f4-85de-4752-b7e4-cb0394fd151a" />
 

עבור קבצי הלמות עם stopwords: 
- מספר שורות ריקות:0
- הצצה לערכי המטריצה:<img width="1112" height="137" alt="image" src="https://github.com/user-attachments/assets/22311d22-341f-455a-a1bc-83e3f00ad84b" />
 

עבור קבצי הלמות בלי stopwords: 
- מספר שורות ריקות:0
- הצצה לערכי המטריצה: <img width="1129" height="140" alt="image" src="https://github.com/user-attachments/assets/75265c99-c597-4d1e-be6f-ff739b55b6d0" />
 

---
## שלב 6 – יצירת ייצוגים סמנטים למסמכים בעזרת SimCSE ו־SBERT
<div dir="rtl" align="right">
**מטרה:** יצירת ייצוגים סמנטיים למסמכי המקור באמצעות שני מודלים מתקדמים : 
1. SimCSE – Unsupervised Contrastive Learning <br>
מודל שלומד מה דומה ומה שונה בין משפטים לפי שלוקח אותו משפט פעמיים ומוסיף לו "רעש" ואז לומד לקרב בין 2 הגרסאות ויודע לייצר ייצוג טוב למשמעות של משפט או מסמך.
2. Sentence-BERT (SBERT)
מודל Transformer שמחזיר וקטור שמתאר את משמעות המשפט.
 
**ביצוע:**  
הרצת סקריפט (make_embeddings.py) שעובר על כל הקבצים בתיקייה `clean_text` שמכילה את הטקסט בלי תגי xml אבל לפני הפרדת סימני הפיסוק ומבצע את הצעדים הבאים:
1. טעינת המודלים: עבור `princeton-nlp/unsup-simcse-bert-base-uncased` -SimCSE 
               עבור `sentence-transformers/all-MiniLM-L6-v2` -SBERT  
2. שמירת רשימה של טקסטים ורשימה של שמות הקבצים, כדי לשייך בין הווקטורים שנוצרים למסמכים.
3. כל מסמך מקבל id ונעשה עליו חלוקה לצ'אנקים כי המודלים מקבלים עד 512 טוקנים, תהליך החלוקה :
   1. פיצול המסמך לפי מילים (מילה= רצף תווים עם רווח לפני ואחרי הרצף)
   2. לוקחים כל 200 מילים וממירים אותם לטוקנים ע"י הtokenizer שמתאים למודל sbert.
   3. אם הרצף שנוצר גדול מ512 מחלקים אותו לחלקים קטנים ומשייכים את כל החלקים לרשימה.
   4. הרשימה נשמרת משויכת בתוך מילון לid.

   
4. יצירת הוקטורים עבור כל הצ'אנקים ע"י כל אחד מהמודלים, זה נעשה בגודל batch 128 .
5. איתור הצ'אנקים של כל מסמך וחישוב ממוצע של הוקטורים של הצ'אנקים וזה וקטור המסמך.
6. שמירת מטריצת הSimCSE בפורמט `npz.` כי זו מטריצה דלילה.
7. שמירת מטריצת הSBERT בפורמט `npz.` כי זו מטריצה דלילה .
8. שמירת שמות הקבצים (= שורות המטריצות ) בפורמט JSON.
9. הרצת הסקריפט (check_matrix_st_sbert.py) בשביל לקבל קצת מידע על המטריצות שנוצרו- הצגת 10 המאפיינים הראשונים ו5 השורות הראשונות.

**תוצאות:**
התקבלה התיקייה embedding_out ובתוכה הקבצים:

- SimCSE-Origin.npz

וקטורים של כל המסמכים לפי מודל SimCSE.

- SBERT-Origin.npz

וקטורים של כל המסמכים לפי מודל SBERT.

- files_mapping.json

רשימה של שמות הקבצים לפי הסדר (כדי לדעת איזה וקטור שייך לאיזה מסמך לפי השורות של המטריצות) .

**ויזואליזציה של המטריצות שנוצרו:** <br> 
עבור המטריצה שנוצרה עם מודל SimCSE:
- מספר מאפיינים:768 
- הצצה לערכי המטריצה: <img width="1118" height="155" alt="image" src="https://github.com/user-attachments/assets/a0c9d5e5-db4a-4a0a-810f-1aa6690260ca" />

עבור המטריצה שנוצרה עם מודל SBERT:
- מספר מאפיינים:384 
- הצצה לערכי המטריצה: <img width="1120" height="152" alt="image" src="https://github.com/user-attachments/assets/2aa8e995-06c8-4fa8-92f4-8b259afae37e" />

</div>

---


## שלב 6 – חישוב Information Gain ו-Mutual Information
**🎯 מטרה:** להעריך את חשיבות כל מאפיין (מילה/למה) בהבחנה בין קטגוריות מסמכים.

**📌 ביצוע:**  
ה־TF-IDF נטען כ־Sparse Matrix. עבור כל עמודת מאפיין חושב Information Gain על בסיס שינוי ה־entropy. במקביל חושב Mutual Information באמצעות `mutual_info_classif`. שתי התוצאות אוחדו ונשמרו בקובץ Excel אחד המציג את דירוג החשיבות של כל מאפיין.

**📈 תוצאה והצדקה:**  
התקבלו שני מדדים משלימים:  
IG — מודד הפחתת אי-ודאות.  
MI — מודד קשר ישיר ומידע משותף בין מאפיין לקטגוריה.  
שילוב שני המדדים מספק תובנה מדויקת לגבי אילו מילים תורמות באמת להבנת הנושא.


---

# ✔ סיכום
הפרויקט כלל בניית צינור מלא של עיבוד טקסט: הורדה, ניקוי, טוקניזציה, למטיזציה, בניית מטריצות TF-IDF ו־BM25, בניית ייצוגים סמנטיים, ולבסוף הערכת מאפיינים לפי חשיבות. הנתונים כעת נקיים, אחידים ומוכנים לביצוע ניתוחים מתקדמים ולבניית מודל אחזור מידע איכותי.

