📘 שלבי העבודה בפרויקט – קורס איחזור מידע
🟦 שלב 1 – הורדת קבצי ה־XML

שם הסקריפט: download_debates.py

🎯 מטרה

להוריד את כל קבצי ה־XML של פרוטוקולי הפרלמנט הבריטי החל מהקובץ debates2023-06-28d.xml ועד הסוף (כ־935 קבצים) לצורך עיבוד בהמשך.

⚙️ מה נעשה

נכתב סקריפט בפייתון המשתמש בספריות requests ו־tqdm להורדה אוטומטית של הקבצים.

הסקריפט:

שואב את רשימת הקבצים מהאתר.

מסנן לפי שם הקובץ המבוקש.

מוריד כל קובץ לתיקייה debates_xml תוך הצגת התקדמות.

מדלג על קבצים שכבר הורדו.

💡 הסיבה לבחירה

שיטה פשוטה, אמינה וחסכונית — מאפשרת הורדה אוטומטית מלאה ללא שימוש ידני בדפדפן, עם הגנות בפני ניתוקים או שגיאות רשת.

📈 תוצאות

כל הקבצים ירדו בהצלחה לתיקייה מקומית.

שמות הקבצים נשמרו במדויק.

שגיאות אפשריות:

ניתוק רשת או Timeout → נפתר באמצעות מנגנון Retry.

קובץ פגום → נבדק לפי גודל, והסקריפט מדלג ומנסה שוב.

🟩 שלב 2 – ניקוי קבצי ה־XML ובדיקת קבצים ריקים

שמות הסקריפטים:

clean_xml_folder.py – ניקוי תגיות XML.

check_empty_files.py – בדיקת קבצים ריקים.

🎯 מטרה

להמיר את קבצי ה־XML לקבצי טקסט נקיים המכילים רק את תוכן הדיבור (ללא תגיות HTML או XML), ולוודא שכל הקבצים שנוצרו אכן תקינים ולא ריקים.

⚙️ מה בוצע
1️⃣ ניקוי תגיות ה־XML

שימוש בספרייה lxml לעיבוד כל קובץ XML.

חילוץ כל הטקסט שבין התגים באמצעות itertext().

הסרת רווחים ותווים מיותרים.

שמירת קובצי טקסט חדשים (.txt) בתיקייה clean_text בשם זהה למקור.

טיפול בשגיאות פרסינג באמצעות קובץ לוג error_log.txt.

2️⃣ בדיקת קבצים ריקים

מעבר על כל קובצי הטקסט בתיקייה.

בדיקה אם גודל הקובץ הוא 0 או שהתוכן קצר מ־10 תווים.

הדפסת רשימת כל הקבצים הריקים או כמעט ריקים.

💡 הסיבה לבחירה

lxml מאפשרת ניתוח מדויק של מבנה XML גם בקבצים מורכבים.

בדיקת קבצים ריקים מבטיחה שקבצים ריקים לא ישפיעו על תוצאות ניתוח המידע בהמשך (TF-IDF, Embeddings).

📈 תוצאות

נוצרו קבצי טקסט נקיים בתיקייה clean_text.

נוצר קובץ לוג לשגיאות פרסינג.

הופקה רשימת קבצים ריקים שנמצאו:

נמצאו 13 קבצים ריקים או כמעט ריקים:
- debates2023-11-14b.txt
- debates2023-12-13b.txt
- debates2023-12-18b.txt
- debates2024-02-06c.txt
- debates2024-05-23b.txt
- debates2024-05-24b.txt
- debates2024-07-17e.txt
- debates2024-12-12b.txt
- debates2025-01-17c.txt
- debates2025-01-22a.txt
- debates2025-03-20a.txt
- debates2025-07-21c.txt
- debates2025-07-22c.txt

🟨 שלב 3 – ניקוי טקסט והפרדת מילים מסימני פיסוק

שם הסקריפט: tokenize_clean_text.py

🎯 מטרה

להבטיח שכל מילה תעמוד בפני עצמה ללא סימני פיסוק צמודים, כך שהמערכת תוכל לבצע חישובי TF-IDF ו־Embeddings בצורה מדויקת.

⚙️ מה בוצע

נכתב סקריפט המשתמש ב־Regex לטוקניזציה מתקדמת.

מזהה מילים בעברית ובאנגלית, כולל תווים מיוחדים כמו גרשיים עבריים (צה"ל, צה״ל).

מפריד סימני פיסוק (. , : ; ? ! ( ) [ ] " - …) כך שכל אחד נשמר כטוקן עצמאי.

שומר את התוצאה בתיקייה tokens תוך שמירה על שמות הקבצים המקוריים.

מטפל בשגיאות קריאה וכתיבה, ומדלג על קבצים בעייתיים.

💡 הסיבה לשיטה

גישה זו שומרת על מבנה המשפט אך מאפשרת ניתוח מילים נפרדות לצורך ניתוח לשוני מדויק, הפחתת רעש ושיפור איכות האינדקס (למשל: ממשלה. לא תיחשב שונה מ־ממשלה).

📈 תוצאות

נוצרו גרסאות “מטוקננות” של כל הקבצים בתיקייה tokens.

כל מילה וסימן פיסוק מופרדים ברווח אחד בלבד.

הנתונים מוכנים כעת לשלב הבא – הסרת stopwords ולממטיזציה (lemmatization).

בשלב יצירת גרסאות הלמות בחרנו להשתמש בכלי spaCy, ובפרט במודל en_core_web_sm.
הספרייה spaCy נבחרה בזכות הדיוק הגבוה שלה בלמטיזציה מבוססת הקשר תחבירי (POS tagging) ובזכות ביצועיה הטובים בעיבוד טקסטים ארוכים באנגלית.
הכלי יודע לזהות את הצורה הבסיסית (lemma) של כל מילה בהתאם להקשרה במשפט, מה שמפחית וריאציות מיותרות ומאפשר ייצוגים עקביים יותר בשלבים הבאים של חישוב TF-IDF ו־Word2Vec.
התהליך בוצע על כל קובצי הטקסט הנקיים מתגי XML, כאשר לכל קובץ נוצר קובץ חדש בשם זהה בתיקייה נפרדת lemmatized_text.

🧩 סיכום ביניים – עד שלב 4.1
🎯 מטרה כללית

להכין את הנתונים הגולמיים (פרוטוקולים מהפרלמנט הבריטי) כך שכל מסמך יוצג בצורה מספרית מדויקת — כדי שנוכל בהמשך להשוות, לנתח ולחשב רלוונטיות בין מסמכים.

🔹 מה בוצע בפועל

הורדת הקבצים – הורדנו מאות קבצי XML מהמאגר הרשמי של הפרלמנט הבריטי.

ניקוי התוכן – הסרנו תגיות XML, תווים מיוחדים וסימני פיסוק, והפקנו טקסט נקי.

הפרדת מילים (Tokenization) – דאגנו שכל מילה וסימן פיסוק יעמדו בנפרד.

יצירת למות (Lemmatization) – הפכנו מילים לצורת השורש שלהן (כמו running → run).

בניית מטריצות TF-IDF ו־BM25 (Word-level) –
לכל מסמך יצרנו וקטור מספרי שמראה אילו מילים מופיעות בו וכמה הן חשובות,
תוך שימוש בצמצום מאפיינים (הסרת stopwords ומילים נדירות).

📊 תוצאות

957 מסמכים

29,162 מאפיינים (מילים שונות)

צפיפות מטריצה: כ־7.5% (מטריצה דלילה — רוב הערכים אפסיים)

נשמרו:

TFIDF_Word.npz

BM25_Word.npz

מיפוי מילים (vocabulary.json)

מיפוי שמות קבצים (files.json)

💡 משמעות התוצאה

עכשיו כל מסמך מוכן לשלב הבא —
למדוד חשיבות מילים בצורה מתקדמת יותר (על בסיס למות)
ולעבור למודלים סמנטיים כמו Word2Vec, GloVe ו־SBERT.

תוצאה: (venv) C:\Users\user\Desktop\שנה ד\איחזור מידע\ir>python build_vectors_word.py --input lemmatized_text --outdir vectors_lemm
נקראו 943 מסמכים מ-lemmatized_text
TF-IDF shape: (943, 22993), צפיפות: 0.078101
Counts shape: (943, 22993)

✅ נשמרו קבצים בתיקייה: C:\Users\user\Desktop\שנה ד\איחזור מידע\ir\vectors_lemm
  - TFIDF_Word.npz + vocab + files
  - BM25_Word.npz  + vocab + files



🧩 סיכום – שלב 4.1 (Lemmas)
🎯 מטרה

לבנות מטריצות TF-IDF ו-BM25 על בסיס הלמות (צורות השורש של המילים),
כדי לבדוק האם ייצוג לפי למות מפחית רעש לשוני ומשפר את איכות המידע לעומת ייצוג לפי מילים רגילות.

⚙️ מה בוצע

נקראו 943 מסמכים מתוך התיקייה lemmatized_text.

נוצרו שתי מטריצות:

TF-IDF_Lemm — חישוב חשיבות של כל למָה בכל מסמך.

BM25_Lemm — גרסה משוקללת עם התאמה לאורך המסמך.

הוסרו stop-words באנגלית, ומילים נדירות מדי (פחות מ-5 הופעות).

נשמרו גם מיפויים (vocabulary.json, files.json) כדי לדעת איזו עמודה שייכת לאיזו למָה ואיזו שורה לאיזה קובץ.

📊 תוצאות

מספר מסמכים: 943

מספר למות ייחודיות: 22,993

צפיפות: ‎≈ 0.078 (דלילה מאוד – כמו שצפוי בטקסט טבעי)

תיקיית פלט: vectors_lemm/

TFIDF_Lemm.npz

BM25_Lemm.npz

קבצי מיפוי נלווים (*.json)

💡 תובנה

בהשוואה לגרסת המילים (Word-level), מתקבלת מטריצה קטנה יותר
(פחות מאפיינים – 22.9K לעומת 29.1K), כי צורות שונות של אותה מילה אוחדו.
זה מאשר שהלמטיזציה הפחיתה כפילויות ואחידות את השפה, מה שמייעל את ניתוח הנתונים בהמשך.

התוצאה: 
נקראו 943 מסמכים מ-lemmatized_text TF-IDF shape: (943, 22993), צפיפות: 0.078101 Counts shape: (943, 22993) ✅ נשמרו קבצים בתיקייה: C:\Users\user\Desktop\שנה ד\איחזור מידע\ir\vectors_lemm - TFIDF_Word.npz + vocab + files - BM25_Word.npz + vocab + files


🧩 סיכום – שלב 4.2 (Word2Vec)
🎯 מטרה

לייצר ייצוג סמנטי לכל מסמך בעזרת Word2Vec, כך שמילים בעלות משמעות דומה יקבלו ייצוגים קרובים במרחב המספרי.

⚙️ מה בוצע

אימון שני מודלי Word2Vec – אחד על מילים רגילות (tokens) ואחד על למות (lemmas).

לכל מסמך חושב וקטור ממוצע של מילותיו (ממד 300).

נבנו שתי גרסאות לכל רמה:

basic – ללא פיסוק, מספרים ותווים מיוחדים.

nostop – בנוסף גם ללא stop-words.

📊 תוצאות

Word-level: ‎957 מסמכים × 300 ממדים.

Lemma-level: ‎943 מסמכים × 300 ממדים.

נשמרו בקבצים בתיקייה embeddings_w2v/:

W2V_Word_basic.npy, W2V_Word_nostop.npy

W2V_Lemm_basic.npy, W2V_Lemm_nostop.npy

ו־w2v_model_word.model, w2v_model_lemm.model



### תיעוד לחישוב מדדים עבור מטריצת TFIDF (Information Gain ו-Mutual Information)

#### 1. **הקדמה:**

בתרגיל זה, המטרה היא לחשב את חשיבות המאפיינים (מילים או למות) במטריצות TFIDF שונות, על ידי שימוש בשני מדדים: **Information Gain** ו-**Mutual Information** (המצוין בשם שיטה שניה במטלה). התוצאה הסופית תהיה קובץ Excel הכולל את חשיבות המאפיינים עבור כל אחד מהמדדים.

#### 2. **מדדים:**

* **Information Gain**:

  * **תיאור**: Information Gain (IG) מודד את כמות המידע שניתן להשיג ממאפיין מסוים על ידי חישוב השינוי בהסתברות של היעד כאשר ידוע המאפיין.
  * **השימוש כאן**: החישוב מבוצע על פי המטריצה של TFIDF, כשהמטרה היא לחשב את ה-IG של כל אחד מהמאפיינים במטריצה ביחס לקטגוריות של הקבצים (כגון נושאים או תגיות).
  * **תהליך חישוב**: תחילה, חושבים את ה-Entropy הכללי של הקטגוריות (תוויות). לאחר מכן, עבור כל מאפיין במטריצה (מילה או למה), מבוצע חישוב ה-Entropy של קבוצות המשויכות לכל ערך של המאפיין. ה-Information Gain מחושב כהפרש בין ה-Entropy הכללי לבין סכום ה-Entropy של כל הקבוצות.

* **Mutual Information**:

  * **תיאור**: Mutual Information (MI) מודד את כמות המידע המשותף בין שני משתנים, במקרה שלנו בין המאפיינים של מטריצת TFIDF לבין התוויות של הקבצים. MI מבוסס על חישוב ההסתברויות המשותפות של המאפיינים והקטגוריות, ומודד עד כמה המידע של מאפיין מסוים תורם להבדלה בין הקטגוריות השונות.
  * **השימוש כאן**: Mutual Information נבחר כמדד נוסף כדי להוסיף עוד פרספקטיבה על חשיבות המאפיינים, זאת משום שהוא מודד את הקשר הסיבתי בין המאפיינים והיעד (קטגוריות). הוא מתאים במיוחד במקרים בהם יש צורך להעריך את עוצמת הקשר בין משתנים שונים, כמו כאן, בו נרצה להבין עד כמה כל מילה או למה במטריצה תורמת להבחנה בין הקטגוריות השונות.

#### 3. **הסיבה לבחירה ב-Mutual Information כמדד נוסף:**

מבין המדדים השונים המוצעים (Gain Ratio, Gini Impurity, Chi-squared statistic, ועוד), ה-**Mutual Information** נבחר מכיוון שהוא:

* **יעיל בהערכת הקשר בין משתנים קטגוריאליים**: כפי שצוין, מטרת החישוב היא להבין את חשיבות המאפיינים (מילים או למות) בהקשר של הקטגוריות. Mutual Information מצוין למקרים של משתנים קטגוריאליים, שכן הוא מודד את הקשר המשותף בין המאפיינים לבין התוויות. לעומת זאת, מדדים כמו **Gini Impurity** או **Gain Ratio** מיועדים בעיקר למודלים של עץ החלטה ואינם בהכרח מייצגים את הקשר בין מאפיינים (כמו מילים) לבין תוויות.
* **הבנה טובה של חפיפות מידע**: ה-Mutual Information לא רק בודק את הקשר, אלא גם מודד את כמות המידע המשותף. זה מאפשר לנו להבין אילו מילים או למות תורמות ביותר להבחנה בין הקטגוריות השונות. מדדים אחרים, כמו **Chi-squared statistic**, אמנם בודקים אם יש קשר בין משתנים, אך אינם מספקים אינדיקציה ישירה לכמות המידע המשותף ביניהם.
* **פשטות ויעילות בחישוב**: Mutual Information הוא יחסית פשוט וקל לשימוש, במיוחד עם ספריות קיימות כמו `mutual_info_classif` בסקikit-learn, מה שמקל על החישוב במטריצות גדולות של TFIDF.

#### 4. **תהליך החישוב:**

* **Information Gain**:

  * המטריצה נטענת כקובץ `.npz` (Sparse Matrix).
  * המילים והמאפיינים נטענים מקובץ `vocabulary.json`.
  * התוויות (קטגוריות) נטענות מקובץ `files.json`.
  * חישוב ה-Information Gain מתבצע לכל מאפיין במטריצה, תוך חישוב ה-Entropy הכללי של התוויות וה-Entropy לכל ערך במאפיין.
* **Mutual Information**:

  * המטריצה נטענת כקובץ `.npz`.
  * התוויות נטענות מקובץ `files.json`.
  * השיטה `mutual_info_classif` מחסיקה את חישוב ה-Mutual Information לכל מאפיין ביחס לתוויות. היא מבוססת על הסתברויות ומספקת את כמות המידע המשותף.

#### 5. **שמירת התוצאות**:

לאחר חישוב שני המדדים (Information Gain ו-Mutual Information), התוצאות נשמרות לקובץ Excel בעזרת הפונקציה `save_to_excel`. כל שורה בקובץ תכיל את המאפיין (מילה או למה), ואת ערך ה-Information Gain ו-Mutual Information עבורו.

#### 6. **סיכום**:

השימוש ב-Information Gain וב-Mutual Information מספק מבט מקיף על החשיבות של כל מאפיין במטריצת TFIDF ומאפשר לנו לבחור את המילים או הלמות החשובות ביותר בהבחנה בין הקטגוריות השונות. הבחירה ב-Mutual Information נועדה להוסיף מימד נוסף של חפיפות מידע ומספקת כלי נוסף להעריך את תרומת המאפיינים למודל האחזור.


דוח מעודכן ⬇️

# דוח עבודה – קורס אחזור מידע / תרגיל 1
תיעוד מלא של שלבי עיבוד הנתונים, ההכנה ובניית הייצוגים למסמכי הפרלמנט הבריטי.

---
## שלב 1 – הורדת קבצי XML
**מטרה:** לאסוף את כל מסמכי המקור הגולמיים (XML) כדי לעבד אותם בהמשך.

**ביצוע:**  
נכתב סקריפט (download_debates.py) המבצע הורדה אוטומטית של כל קובץ XML מהאתר: https://www.theyworkforyou.com/pwdata/scrapedxml/debates/ החל מהקובץ debates2023-06-28d.xml ועד סוף הארכיון. <br> 
הסקריפט שולף את רשימת הקבצים מהאתר הרשמי, מסנן לפי תבנית שם, ומוריד כל מסמך לתיקייה מסודרת. 

**תוצאות:**  
התקבלה ספרייה בשם debates_xml שמכילה 956 קבצים.

---

## שלב 2 – ניקוי קבצי הXML
### שלב 2.1 – ניקוי תגיות XML
**מטרה:** להפיק טקסט נקי מתגיות ולהבטיח שרק קבצים בעלי תוכן יעברו לעיבוד.

**ביצוע:**  
נכתב סקריפט (clean_tags.py) שמעבד את קבצי הxml בעזרת הספרייה `lxml` ומשאיר רק את הטקסט שהיה בין התגיות. 
לאחר השלמת פעולת הניקוי הורץ קוד שבודק כמה קבצים ריקים יש - אין בהן תוכן, ואת שמותיהם ואותם מחקנו כי לא נותנים שום מידע.  

**תוצאות:**  
התקבלה ספרייה בשם clean_text שמכילה את הקבצים הנקיים מתגים בלי 13 קבצים הריקים ששמותיהם:

 debates2023-11-14b.txt <br>
 debates2023-12-13b.txt <br>
 debates2023-12-18b.txt <br>
 debates2024-02-06c.txt <br>
 debates2024-05-23b.txt <br>
 debates2024-05-24b.txt <br>
 debates2024-07-17e.txt <br>
 debates2024-12-12b.txt <br>
 debates2025-01-17c.txt <br>
 debates2025-01-22a.txt <br>
 debates2025-03-20a.txt <br>
 debates2025-07-21c.txt <br>
 debates2025-07-22c.txt <br>

 סה"כ יש 943 קבצים בתיקייה לעיבוד עתידי.


### שלב 2.2 – טוקניזציה (Tokenization)
**מטרה:** לפרק את הטקסט למילים וסימני פיסוק נפרדים, כדי של מילה תהיה משמעות בפני עצמה ללא סימני פיסוק צמודים כך שיהיה אפשר לבצע חישובים בצורה מדוייקת.

**ביצוע:**  
נכתב סקריפט (tokenize_clean_text.py) שעובר על כל אחד מהקבצים שנוצרו בשלב 2.1 ומשתמש במנגנון המובנה של פייתון str.translate וב־string.punctuation כדי לזהות את כל סימני הפיסוק הנפוצים: !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~<br> 
לכל סימן פיסוק מוגדר תרגום המכניס רווח לפני ואחרי הסימן.
כך, בעת העיבוד, כל סימני הפיסוק מופרדים אוטומטית מהמילים.

**תוצאות:**  
התקבלה ספרייה בשם tokens שמכילה את הטקסט עם סימני פיסוק מופרדים.<br> 
הערה: מילים עם s השייכות שמופיעות כך: s' לא הופרדו כי זה חלק מהמילה ומציין שייכות ולא כמו סימני פיסוק אחרים שהם חלק מהמשפט ולא מהמילה. 


---

## שלב 3 – Lemmatization
**מטרה:** לצמצם וריאציות לשוניות על ידי המרה לצורות בסיס (lemmas), ולהקטין את מספר המאפיינים.

**ביצוע:**
כתיבת סקריפט (lemma.py) שעובר על כל הקבצים בתיקייה tokens ומשתמש בכלי spaCy עם המודל en_core_web_sm שמזהה את תפקיד המילה במשפט (POS) וכך עוזר ליצור את הלמה של המילה.

**תוצאות:**
התקבלה תיקייה lemmatized_text שהמילים בכל קובץ מופיעות בצורת הלמה שלהן.

---

## שלב 4 - בניית וקטורים לייצוג TF-IDF עם BM25\Okapi

**מטרה:**  
יצירת וקטור מאפיינים לכל מסמך. כל מסמך ממופה לווקטור מספרי המתאר את המילים שמופיעות בו ואת החשיבות היחסית שלהן באמצעות TF-IDF עם BM25\Okapi.

**ביצוע:**  
כתיבת סקריפט (`build_vectors_word.py`) שעובר על כל הקבצים פעמיים: פעם אחת בתיקייה `tokens` ופעם שנייה בתיקייה `lemmatized_text`. הסקריפט מבצע את הצעדים הבאים:

1. שמירת רשימה של טקסטים ורשימה של שמות הקבצים, כדי לשייך בין הווקטורים שנוצרים למסמכים.
2. המרת הטקסטים למטריצות שבהן כל תא מכיל את כמות הפעמים שהמילה מופיעה במסמך (TF) באמצעות הפונקציה `CountVectorizer` מתוך `scikit-learn`:

```python
from sklearn.feature_extraction.text import CountVectorizer

count_vec = CountVectorizer(
    input="content",
    analyzer="word",
    stop_words="english",
    min_df=5
)
```
| פרמטר                  | משמעות                             |
| ---------------------- | ---------------------------------- |
| `input="content"`      | מציין שהקלט הוא טקסט גולמי         |
| `analyzer="word"`      | מפצל את הטקסט למילים               |
| `stop_words="english"` | מסיר stopwords באנגלית             |
| `min_df=5`             | מסיר מילים שמופיעות פחות מ-5 פעמים |

3. חישוב כל תא במטריצה באמצעות BM25\Okapi ידנית עם הפרמטרים `k=1.5` ו-`b=0.75`.  
4. שמירת המטריצה שנוצרה בפורמט `.npz` כדי לשמור מטריצה דלילה.  
5. שמירת המילים עצמם בפורמט JSON, כאשר המפתח הוא המילה והערך הוא מספר הפעמים שהיא מופיעה.  
6. שמירת שמות הקבצים בפורמט JSON.

---

**תוצאות:**  
לאחר שתי הרצות התקבלו שתי תיקיות: `vectors_word` ו-`vectors_lemm`, שבהן יש את הקבצים הבאים:

- **מטריצת TF-IDF:**  
   `TFIDF-Word.npz` / `TFIDF-Lemm.npz`

- **המילים בעמודות המטריצה:**  
   `TFIDF-Word_vocabulary.json` / `TFIDF-Lemm_vocabulary.json`

- **שמות המסמכים (השורות במטריצה):**  
   `TFIDF-Word_files.json` / `TFIDF-Lemm_files.json`


---



## שלב 4 – בניית ייצוגים סמנטיים (Word2Vec)
**🎯 מטרה:** לייצג מסמכים לפי המשמעות, לא רק לפי שכיחות מילים.

**📌 ביצוע:**  
אומנו שני מודלי Word2Vec: אחד על טוקנים רגילים ואחד על למות. לכל מסמך נבנה וקטור בגודל 300 על ידי ממוצע הווקטורים של המילים המופיעות בו. נבנו שתי גרסאות: basic (ללא פיסוק ומספרים) ו־nostop (גם ללא stopwords). כל המודלים והווקטורים נשמרו בתיקייה מסודרת.

**📈 תוצאה והצדקה:**  
התקבלו שני סטים של ייצוגים סמנטיים: word-level ו־lemma-level. הווקטורים מאפשרים לזהות דמיון לפי משמעות ולא רק דמיון מילולי. השלב חיוני עבור משימות כמו clustering או חיפוש דמיון סמנטי.

---

## שלב 5 – חישוב Information Gain ו-Mutual Information
**🎯 מטרה:** להעריך את חשיבות כל מאפיין (מילה/למה) בהבחנה בין קטגוריות מסמכים.

**📌 ביצוע:**  
ה־TF-IDF נטען כ־Sparse Matrix. עבור כל עמודת מאפיין חושב Information Gain על בסיס שינוי ה־entropy. במקביל חושב Mutual Information באמצעות `mutual_info_classif`. שתי התוצאות אוחדו ונשמרו בקובץ Excel אחד המציג את דירוג החשיבות של כל מאפיין.

**📈 תוצאה והצדקה:**  
התקבלו שני מדדים משלימים:  
IG — מודד הפחתת אי-ודאות.  
MI — מודד קשר ישיר ומידע משותף בין מאפיין לקטגוריה.  
שילוב שני המדדים מספק תובנה מדויקת לגבי אילו מילים תורמות באמת להבנת הנושא.

---

# ✔ סיכום
הפרויקט כלל בניית צינור מלא של עיבוד טקסט: הורדה, ניקוי, טוקניזציה, למטיזציה, בניית מטריצות TF-IDF ו־BM25, בניית ייצוגים סמנטיים, ולבסוף הערכת מאפיינים לפי חשיבות. הנתונים כעת נקיים, אחידים ומוכנים לביצוע ניתוחים מתקדמים ולבניית מודל אחזור מידע איכותי.

